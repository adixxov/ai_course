{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T14:23:40.975759Z",
     "start_time": "2024-09-25T14:23:22.589592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install virtualenv\n",
    "!virtualenv ai_course_env"
   ],
   "id": "e22754660d21acf3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Collecting virtualenv\r\n",
      "  Downloading virtualenv-20.26.5-py3-none-any.whl.metadata (4.5 kB)\r\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv)\r\n",
      "  Downloading distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Requirement already satisfied: filelock<4,>=3.12.2 in /home/ahmeddoha/PycharmProjects/venvs/Applied_AI_Course_venv/lib/python3.10/site-packages (from virtualenv) (3.16.0)\r\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /home/ahmeddoha/PycharmProjects/venvs/Applied_AI_Course_venv/lib/python3.10/site-packages (from virtualenv) (4.2.2)\r\n",
      "Downloading virtualenv-20.26.5-py3-none-any.whl (6.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.0/6.0 MB\u001B[0m \u001B[31m212.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\r\n",
      "Installing collected packages: distlib, virtualenv\r\n",
      "Successfully installed distlib-0.3.8 virtualenv-20.26.5\r\n",
      "created virtual environment CPython3.10.14.final.0-64 in 7764ms\r\n",
      "  creator CPython3Posix(dest=/home/ahmeddoha/PycharmProjects/ai_course/class_04/ai_course_env, clear=False, no_vcs_ignore=False, global=False)\r\n",
      "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/ahmeddoha/.local/share/virtualenv)\r\n",
      "    added seed packages: pip==24.2, setuptools==75.1.0, wheel==0.44.0\r\n",
      "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install accelerate==0.33.0\n",
    "!pip install apify-client==1.6.4\n",
    "!pip install arxiv==2.1.0\n",
    "!pip install duckduckgo_search==5.2.1\n",
    "!pip install faiss-cpu==1.8.0\n",
    "!pip install google-api-python-client==2.124.0\n",
    "!pip install google-cloud-aiplatform==1.60.0\n",
    "!pip install google-generativeai==0.4.1\n",
    "!pip install gpt4all==2.3.2\n",
    "!pip install gradio==3.50.2\n",
    "!pip install huggingface-hub==0.24.5\n",
    "!pip install jupyter\n",
    "!pip install lanarky==0.8.6\n",
    "!pip install langchain-anthropic==0.1.4\n",
    "!pip install langchain-chroma==0.1.2\n",
    "!pip install langchain-cli==0.0.21\n",
    "!pip install langchain-cohere==0.1.1\n",
    "!pip install langchain-core==0.1.42\n",
    "!pip install langchain-experimental==0.0.55\n",
    "!pip install langchain-google-genai==1.0.1\n",
    "!pip install langchain-google-vertexai==0.1.2\n",
    "!pip install langchain-openai==0.1.1\n",
    "!pip install langchain[docarray]==0.1.13\n",
    "!pip install langchainhub==0.1.20\n",
    "!pip install langserve[all]==0.0.51\n",
    "!pip install numexpr==2.10.0\n",
    "!pip install opencv-python==4.9.0.80\n",
    "!pip install pandoc==1.1.0  # needs system install as well!\n",
    "!pip install pdf2image==1.17.0\n",
    "!pip install pdfminer.six==20231228\n",
    "!pip install pikepdf==8.15.1\n",
    "!pip install pillow_heif==0.16.0\n",
    "!pip install pymupdf==1.24.1\n",
    "!pip install pypdf==4.1.0\n",
    "!pip install replicate==0.30.1\n",
    "!pip install ruff==0.3.4\n",
    "!pip install sentence-transformers==2.6.1\n",
    "!pip install streamlit==1.37.0\n",
    "!pip install tiktoken==0.6.0\n",
    "!pip install unstructured==0.15.0\n",
    "!pip install wikipedia==1.4.0\n",
    "!pip install wolframalpha==5.0.0"
   ],
   "id": "760dce1758cf227c"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "# config.py",
   "id": "f1aa8284874c2c1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "OPENAI_API_KEY = \"OPENAI_API_KEY\"\n",
    "# I'm omitting all other keys\n",
    "def set_environment():\n",
    "    variable_dict = globals().items()\n",
    "    for key, value in variable_dict:\n",
    "        if \"API\" in key or \"ID\" in key:\n",
    "            os.environ[key] = value"
   ],
   "id": "581740e2a0bdd498"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "# utils.py",
   "id": "17c0522da2506ec6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"Utility functions and constants.\n",
    "\n",
    "I am having some problems caching the memory and the retrieval. When\n",
    "I decorate for caching, I get streamlit init errors.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Any\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders.epub import UnstructuredEPubLoader\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders.word_document import UnstructuredWordDocumentLoader\n",
    "from langchain_core.documents import Document\n",
    "from streamlit.logger import get_logger\n",
    "\n",
    "logging.basicConfig(encoding=\"utf-8\", level=logging.INFO)\n",
    "LOGGER = get_logger(__name__)\n",
    "\n",
    "\n",
    "def init_memory():\n",
    "    \"\"\"Initialize the memory for contextual conversation.\n",
    "\n",
    "    We are caching this, so it won't be deleted\n",
    "     every time, we restart the server.\n",
    "    \"\"\"\n",
    "    return ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\", return_messages=True, output_key=\"answer\"\n",
    "    )\n",
    "\n",
    "\n",
    "LOGGER.info(\"init memory\")\n",
    "MEMORY = init_memory()\n",
    "\n",
    "\n",
    "class EpubReader(UnstructuredEPubLoader):\n",
    "    def __init__(self, file_path: str | list[str], **unstructured_kwargs: Any):\n",
    "        super().__init__(file_path, **unstructured_kwargs, mode=\"elements\", strategy=\"fast\")\n",
    "\n",
    "\n",
    "class DocumentLoaderException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class DocumentLoader(object):\n",
    "    \"\"\"Loads in a document with a supported extension.\"\"\"\n",
    "\n",
    "    supported_extensions = {\n",
    "        \".pdf\": PyPDFLoader,\n",
    "        \".txt\": TextLoader,\n",
    "        \".epub\": EpubReader,\n",
    "        \".docx\": UnstructuredWordDocumentLoader,\n",
    "        \".doc\": UnstructuredWordDocumentLoader,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_document(temp_filepath: str) -> list[Document]:\n",
    "    \"\"\"Load a file and return it as a list of documents.\n",
    "\n",
    "    Doesn't handle a lot of errors at the moment.\n",
    "    \"\"\"\n",
    "    ext = pathlib.Path(temp_filepath).suffix\n",
    "    loader = DocumentLoader.supported_extensions.get(ext)\n",
    "    if not loader:\n",
    "        raise DocumentLoaderException(\n",
    "            f\"Invalid extension type {ext}, cannot load this type of file\"\n",
    "        )\n",
    "\n",
    "    loaded = loader(temp_filepath)\n",
    "    docs = loaded.load()\n",
    "    logging.info(docs)\n",
    "    return docs\n"
   ],
   "id": "17537da6a9c3a056"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "# chat_with_documents.py",
   "id": "c4eb502964a04a39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"Chat with retrieval and embeddings.\"\"\"\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# from class04_llm_rag_auffarth.chapter5.config import set_environment\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain.chains.flare.base import FlareChain\n",
    "from langchain.chains.moderation import OpenAIModerationChain\n",
    "from langchain.chains.sequential import SequentialChain\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_community.vectorstores.docarray import DocArrayInMemorySearch\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from class04_llm_rag_auffarth.chapter5.chat_with_retrieval.utils import LOGGER, MEMORY, load_document\n",
    "\n",
    "set_environment()\n",
    "\n",
    "LOGGER.info(\"setup LLM\")\n",
    "# Setup LLM and QA chain; set temperature low to keep hallucinations in check\n",
    "LLM = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "\n",
    "LOGGER.info(\"configure_retriever\")\n",
    "\n",
    "\n",
    "def configure_retriever(docs: list[Document], use_compression: bool = False) -> BaseRetriever:\n",
    "    \"\"\"Retriever to use.\"\"\"\n",
    "    # Split each document documents:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create embeddings and store in vectordb:\n",
    "    # embeddings = OpenAIEmbeddings()\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    # Create vectordb with single call to embedding model for texts:\n",
    "    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5, \"fetch_k\": 7, \"include_metadata\": True},\n",
    "    )\n",
    "    if not use_compression:\n",
    "        return retriever\n",
    "\n",
    "    embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.2)\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=embeddings_filter,\n",
    "        base_retriever=retriever,\n",
    "    )\n",
    "\n",
    "\n",
    "def configure_chain(retriever: BaseRetriever, use_flare: bool = True) -> Chain:\n",
    "    \"\"\"Configure chain with a retriever.\n",
    "\n",
    "    Passing in a max_tokens_limit amount automatically\n",
    "    truncates the tokens when prompting your llm!\n",
    "    \"\"\"\n",
    "    output_key = \"response\" if use_flare else \"answer\"\n",
    "    MEMORY.output_key = output_key\n",
    "    params = dict(\n",
    "        llm=LLM,\n",
    "        retriever=retriever,\n",
    "        memory=MEMORY,\n",
    "        verbose=True,\n",
    "        max_tokens_limit=4000,\n",
    "    )\n",
    "    if use_flare:\n",
    "        # different set of parameters and init\n",
    "        return FlareChain.from_llm(**params)\n",
    "    return ConversationalRetrievalChain.from_llm(**params)\n",
    "\n",
    "\n",
    "def configure_retrieval_chain(\n",
    "    uploaded_files,\n",
    "    use_compression: bool = False,\n",
    "    use_flare: bool = False,\n",
    "    use_moderation: bool = False,\n",
    ") -> Chain:\n",
    "    \"\"\"Read documents, configure retriever, and the chain.\"\"\"\n",
    "    docs = []\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "    for file in uploaded_files:\n",
    "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
    "        with open(temp_filepath, \"wb\") as f:\n",
    "            f.write(file.getvalue())\n",
    "        docs.extend(load_document(temp_filepath))\n",
    "\n",
    "    retriever = configure_retriever(docs=docs, use_compression=use_compression)\n",
    "    chain = configure_chain(retriever=retriever, use_flare=use_flare)\n",
    "    if not use_moderation:\n",
    "        return chain\n",
    "\n",
    "    input_variables = [\"user_input\"] if use_flare else [\"chat_history\", \"question\"]\n",
    "    moderation_input = \"response\" if use_flare else \"answer\"\n",
    "    moderation_chain = OpenAIModerationChain(input_key=moderation_input)\n",
    "    return SequentialChain(chains=[chain, moderation_chain], input_variables=input_variables)\n"
   ],
   "id": "c11306926dd09ec6"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "# app.py",
   "id": "9bc78bdd0ca92e1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install --upgrade docarray",
   "id": "6ff4ba869ed2dcfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"Document loading functionality.\n",
    "\n",
    "Run like this:\n",
    "\n",
    "PYTHONPATH=. streamlit run /class_04/chat_with_retrieval/app.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit.external.langchain import StreamlitCallbackHandler\n",
    "\n",
    "# from class_04.chat_with_retrieval.chat_with_documents import configure_retrieval_chain\n",
    "# from class_04.chat_with_retrieval.utils import LOGGER, MEMORY, DocumentLoader\n",
    "\n",
    "LOGGER.info(\"Show title\")\n",
    "st.set_page_config(page_title=\"LangChain: Chat with Documents\", page_icon=\"🦜\")\n",
    "st.title(\"🦜 LangChain: Chat with Documents\")\n",
    "\n",
    "\n",
    "LOGGER.info(\"Upload files\")\n",
    "uploaded_files = st.sidebar.file_uploader(\n",
    "    label=\"Upload files\",\n",
    "    type=list(DocumentLoader.supported_extensions.keys()),\n",
    "    accept_multiple_files=True,\n",
    ")\n",
    "if not uploaded_files:\n",
    "    st.info(\"Please upload documents to continue.\")\n",
    "    st.stop()\n",
    "\n",
    "# use compression by default:\n",
    "use_compression = st.checkbox(\"compression\", value=False)\n",
    "use_flare = st.checkbox(\"flare\", value=False)\n",
    "use_moderation = st.checkbox(\"moderation\", value=False)\n",
    "\n",
    "LOGGER.info(\"Configure chain\")\n",
    "CONV_CHAIN = configure_retrieval_chain(\n",
    "    uploaded_files,\n",
    "    use_compression=use_compression,\n",
    "    use_flare=use_flare,\n",
    "    use_moderation=use_moderation,\n",
    ")\n",
    "\n",
    "LOGGER.info(\"Clear button\")\n",
    "if st.sidebar.button(\"Clear message history\"):\n",
    "    MEMORY.chat_memory.clear()\n",
    "\n",
    "avatars = {\"human\": \"user\", \"ai\": \"assistant\"}\n",
    "\n",
    "if len(MEMORY.chat_memory.messages) == 0:\n",
    "    st.chat_message(\"assistant\").markdown(\"Ask me anything!\")\n",
    "\n",
    "for msg in MEMORY.chat_memory.messages:\n",
    "    st.chat_message(avatars[msg.type]).write(msg.content)\n",
    "\n",
    "LOGGER.info(\"Chat interface\")\n",
    "container = st.container()\n",
    "assistant = st.chat_message(\"assistant\")\n",
    "if user_query := st.chat_input(placeholder=\"Give me 3 keywords for what you have right now\"):\n",
    "    st.chat_message(\"user\").write(user_query)\n",
    "    stream_handler = StreamlitCallbackHandler(container)\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        if use_flare:\n",
    "            params = {\"user_input\": user_query}\n",
    "        else:\n",
    "            params = {\n",
    "                \"question\": user_query,\n",
    "                \"chat_history\": MEMORY.chat_memory.messages,\n",
    "            }\n",
    "        response = CONV_CHAIN.invoke(\n",
    "            {\"question\": user_query, \"chat_history\": MEMORY.chat_memory.messages},\n",
    "            callbacks=[stream_handler],\n",
    "        )\n",
    "        # Display the response from the chatbot\n",
    "        if response:\n",
    "            container.markdown(response)\n"
   ],
   "id": "72957b711fe9be09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
