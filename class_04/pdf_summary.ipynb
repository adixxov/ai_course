{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# setting the environment variables\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T13:44:07.796824Z",
     "start_time": "2024-04-09T13:44:07.786562Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "\"<kindergarten_abstract>Large language models can think better by following a chain of thoughts. This helps them solve math problems and other tricky tasks.</kindergarten_abstract>\\n\\n<moosewood_methods>\\nRecipe for Chain-of-Thought Prompting\\n\\nIngredients:\\n- Large language model (preferably one with billions of parameters)\\n- Chain of thought demonstrations\\n- Math word problems, commonsense questions, and symbolic reasoning tasks\\n\\nInstructions:\\n1. Preheat the language model by providing it with a few chain of thought exemplars for each task.\\n2. Let the model generate a series of reasoning steps to reach the final answer for each problem.\\n3. Evaluate the model's performance on arithmetic, commonsense, and symbolic reasoning benchmarks.\\n4. Enjoy improved performance on challenging tasks, especially with larger language models.\\n\\nNote: For best results, ensure the language model is well-fed with quality data and supervision.\\n\\n</moosewood_methods>\\n\\n<homer_results>\\nOh mighty language models, with chains of thought you soar,\\nSolving math problems and reasoning tasks galore.\\nEmerging abilities at scale, you shine bright,\\nUnveiling new ways to tackle challenges with might.\\n\\nFrom LaMDA to PaLM, your powers unfold,\\nGuiding us through complex problems with stories untold.\\nChain-of-thought prompting, a method so grand,\\nUnlocks the potential of language models across the land.\\n\\nThe GSM8K benchmark bows to your might,\\nState-of-the-art performance, a glorious sight.\\nMath word problems and commonsense too,\\nYou conquer them all, with wisdom anew.\\n\\nSo raise your voices, oh models of grand scale,\\nFor chain-of-thought reasoning, we shall hail.\\nIn the realm of NLP, you reign supreme,\\nA testament to the power of a language model dream.\\n\\n</homer_results>\\n\\n<grouchy_critique>\\nThis paper on chain-of-thought prompting in large language models is another example of researchers chasing after the latest trend without considering the practical implications. While the concept may sound intriguing, the implementation leaves much to be desired.\\n\\nThe authors fail to address the real-world applicability of their method, focusing instead on benchmark performance that may not translate to actual use cases. Additionally, the reliance on large language models with billions of parameters raises concerns about computational efficiency and environmental impact.\\n\\nFurthermore, the lack of thorough validation and robustness testing undermines the credibility of the results. Without proper scrutiny and validation, the claims made by the authors cannot be taken at face value.\\n\\nIn conclusion, while the idea of chain-of-thought prompting may have merit, this paper falls short in providing a comprehensive and practical solution for improving reasoning in language models.\\n\\n</grouchy_critique>\""
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"2201.11903\", load_max_docs=2).load()\n",
    "\n",
    "# Create chain\n",
    "prompt = hub.pull(\"hwchase17/anthropic-paper-qa\")\n",
    "model = ChatAnthropic(model=\"claude-2\", max_tokens=10000)\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke\n",
    "chain.invoke({\"text\": docs[0].page_content})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T13:56:56.949985Z",
     "start_time": "2024-04-09T13:56:47.032980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptTemplate(input_variables=['text'], metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'anthropic-paper-qa', 'lc_hub_commit_hash': '0b8e75415e4d1314431e2a22176dce33c65375d4b3be7a2e21c91819da6dfbf7'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='Here is an academic paper: <paper>{text}</paper>\\n\\nPlease do the following:\\n1. Summarize the abstract at a kindergarten reading level. (In <kindergarten_abstract> tags.)\\n2. Write the Methods section as a recipe from the Moosewood Cookbook. (In <moosewood_methods> tags.)\\n3. Compose a short poem epistolizing the results in the style of Homer. (In <homer_results> tags.)\\n4. Write a grouchy critique of the paper from a wizened PI. (In <grouchy_critique> tags.)'))])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T14:01:48.216490Z",
     "start_time": "2024-04-09T14:01:48.203056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without proper scrutiny and validation, the claims made by the authors cannot be taken at face value.\n",
      "\n",
      "In conclusion, while the idea of chain-of-thought prompting may have merit, this paper falls short in providing a comprehensive and practical solution for improving reasoning in language models.\n",
      "\n",
      "</grouchy_critique\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Without proper scrutiny and validation, the claims made by the authors cannot be taken at face value.\\n\\nIn conclusion, while the idea of chain-of-thought prompting may have merit, this paper falls short in providing a comprehensive and practical solution for improving reasoning in language models.\\n\\n</grouchy_critique\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:22.669188Z",
     "start_time": "2024-04-09T14:05:22.656718Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
